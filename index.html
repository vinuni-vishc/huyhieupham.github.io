<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <title>Huy Hieu Pham</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Huy Hieu Pham</name>
              </p>
              <p> I am a staff Research cientist at Vingroup Big Data Institute (<a href="http://vinbdi.org/">VinBDI</a>), where I work on Computer Vision and Artificial Intelligence (AI). I received my Ph.D. degree in Computer Science at the <a href="https://www.irit.fr/?lang=en">Toulouse Computer Science Research Institute (IRIT)</a> and <a href="https://www.cerema.fr/fr">Toulouse Cerema Research Center</a>, France. My research focuses on Computer Vision, Machine Learning, and Deep Learning. In particular, I design and optimize high-performance deep learning models for solving the security problems in public transport, e.g. the problem of human action recognition and behavior analytics in videos. My research works were advised by <a href="https://www.researchgate.net/profile/Louahdi_Khoudour">Dr. HDR. Louahdi Khoudour</a>, <a href="https://www.irit.fr/~Alain.Crouzil/">Dr. Alain Crouzil</a>, <a href="https://scholar.google.com/citations?user=0AoOLS0AAAAJ&hl=es">Dr. Pablo Zegers</a>, and <a href="https://scholar.google.com/citations?user=FsE86kwAAAAJ&hl=en">Prof. Sergio A. Velastin</a>. 
              </p>
              <p>
                I spent half a year at the <a href="http://institut-clement-ader.org/">ICA laboratory</a>, <a href="https://www.imt-mines-albi.fr/">École des Mines d'Albi</a> under the supervision of <a href="http://perso.mines-albi.fr/~jeanjose/">Prof. Jean-José Orteu</a> for working on a research project. My work was part of a dynamic multi-partner robotized airplane inspection project, called <a href="https://www.youtube.com/watch?v=YLikbi48yEg">Air-Cobot</a>, lead by <a href="https://www.akka-technologies.com/fr">AKKA Technologies</a> and <a href="AIRBUS group">AIRBUS group</a>.
              </p>
			  
	      <p>
	      Previously, I did my bachelors at the <a href="https://en.hust.edu.vn/">Hanoi University of Science and Technology (HUST)</a>. My graduate internship was done at <a href="https://www.mica.edu.vn/">International Research Institute MICA</a>, Hanoi, Vietnam et <a href="http://www.grenoblecognition.fr/index.php/le-pole/unites-contituantes/57-les-unites/172-agim-andrologie-gerontologie-inflammation-modelisation"> AGIM laboratory</a>, <a href="https://www.univ-grenoble-alpes.fr/english/">Université Grenoble Alpes</a>, Grenoble, France. The main goal of this project is to develop a computer vision algorithm based on electrode matrix and mobile Kinect for detecting obstacles and warning to visually impaired people.
	      </p>  
			  
              <p align=center>
		<a href="data/Huy_Hieu_PHAM_Doctoral_Thesis.pdf">Ph.D. Thesis</a> &nbsp/&nbsp
                <a href="data/HieuPham-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=mXcFcNkAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
		<a href="https://github.com/huyhieupham/">GitHub</a> &nbsp/&nbsp
		<a href="https://www.linkedin.com/in/pham-huy-hieu-25615236/">LinkedIn</a> &nbsp/&nbsp
		<a href="https://twitter.com/Hieuhuy">Twitter</a>
              </p>
            </td>
            <td width="33%">
              <img src="images/huyhieupham.jpg">
            </td>
          </tr>
		
         </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
              <p>
                I'm interested in Image Processing, Computer Vision, Machine Learning, Statistics, and Optimization. Much of my research is about understanding the physical world (shape, depth, motion, object detection and recognition) from images and videos. I have also worked in Medical Imaging projects.
              </p>
            </td>
          </tr>
	 </table>
	      
	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>News</heading>
		<p>    
		   <span>&#x25cf;</span> &nbsp; <b>[Sep. 2019]</b> &nbsp;  I just successfully defended my <a href="data/Huy_Hieu_PHAM_Doctoral_Thesis.pdf">Ph.D. thesis</a> at the Computer Science Research Institute of Toulouse (IRIT), France. The presentation is available <a href="data/PhD_Presentation_Final_Version_Hieu_Pham.pdf">here</a>. I will back to Hanoi, Vietnam and work as a Research Scientist in Computer Vision and Artificial Intelligence (AI) at the Vingroup Big Data Institute (VinBDI) starting from October 2019. My work focuses on applying the latest advances in Machine Learning & Deep Learning for Medical Imaging Analysis.
		</p>    
		    
		<p>    
		   <span>&#x25cf;</span> &nbsp; <b>[Aug. 2019]</b> &nbsp;  I gave an <a href="data/ICIAR_2019_Hieu_Pham.pdf">oral presentation</a> at the 16th International Conference on Image Analysis and Recognition (<a href="https://www.aimiconf.org/iciar19/">ICIAR'2019</a>).  
		</p>		    
		    
		<p>    
		   <span>&#x25cf;</span> &nbsp; <b>[July 2019]</b> &nbsp; In our recent study, we showed that a simple deep neural network is able to learn and predict 3D human poses from 2D keypoints obtained from RGB images. The full paper about this research is available on <a href="https://arxiv.org/pdf/1907.06968v1.pdf">arXiv</a>.   
		</p> 
	
		<p>    
		   <span>&#x25cf;</span> &nbsp; <b>[July 2019]</b> &nbsp;  I gave a <a href="data/Poster_SI_IRIT_17_July_2019.pdf">poster presentation</a> at the annual seminar of Toulouse Computer Science Research Institute (IRIT), France. The presentation was about training a deep learning neural network for predicting 3D human poses from their 2D keypoint detections on RGB images.   
		</p>
	      <p>
	      <span>&#x25cf;</span> &nbsp; <b>[July 2019]</b> &nbsp; A preprint of my ICIAR2019 paper is available on <a href="https://arxiv.org/pdf/1907.03520.pdf">arXiv</a>.
	      </p>
	      <p>
              		<span>&#x25cf;</span> &nbsp; <b>[May 2019]</b> &nbsp; My paper that was submitted to the <a href="https://www.aimiconf.org/iciar19/">16th International Conference on Image Analysis and Recognition</a> got accepted. I will be in Waterloo, Canada from August 27th to 29th, 2019 to give my talk about a new deep learning model for human action analysis.
              </p>
		    	    
	      <p>
		        <span>&#x25cf;</span> &nbsp; <b>[Apr. 2019]</b> &nbsp; Our new paper titled :"Spatio-Temporal Image Representation of 3D Skeletal Movements for View-Invariant Action Recognition with Deep Convolutional Neural Networks has been accepted for publication by the Special Issue "Deep Learning-Based Image Sensors". A preprint of the paper can be downloaded <a href="https://www.preprints.org/manuscript/201903.0086/v1">here</a>.
              </p>
		        
		<p>
		     <span>&#x25cf;</span> &nbsp; <b>[Aug. 2018]</b> &nbsp; We publish <a href="https://sites.google.com/site/hhpham172/image-processing-and-computer-vision/tisseo-cerema-dataset"> a new RGB-D dataset</a> for passenger behavior analysis at a metro station in Toulouse, France using an inexpensive Microsoft Kinect v2 sensor. This dataset was originally recorded for an academia project, and it must be used only for the purpose of research. The dataset was created with the collaboration between the Cerema Research Center and the Tisséo network (a public transportation network for Toulouse city) – to improve security and safety in public transport. 
		</p>
            
		<p> 
			<span>&#x25cf;</span> &nbsp; <b>[July 2018]</b> &nbsp; My new paper about improving ResNet architecture for human action recognition (HAR) with RGB-D data has been accepted with revision for publication by the <a href="http://digital-library.theiet.org/content/journals/iet-cvi"> IET Computer Vision Journal</a>.
			</p>
		 <p>
		<span>&#x25cf;</span> &nbsp; <b>[May 2018]</b> &nbsp; My paper titled: "Exploiting Inception-ResNet networks for human action recognition (HAR) from skeletal data" has been accepted for oral presentation at the <a href="https://2018.ieeeicip.org/">25th IEEE International Conference on Image Processing (ICIP 2018)</a>.
		</p>
		<p>
		<span>&#x25cf;</span> &nbsp; <b>[Jun. 2018 ]</b> &nbsp; My paper: "Exploiting deep residual networks for human action recognition from skeletal data" has been accepted for publication in the <a href="https://www.sciencedirect.com/science/article/pii/S1077314218300389"> Computer Vision and Image Understanding Journal</a>.
		</p> 
		<p>
		<span>&#x25cf;</span> &nbsp; <b>[Nov. 2017]</b> &nbsp; I am very honored to give a talk about Deep Learning for Image Recognition to the Ph.D. and Master's students at the Computer Science Department, <a href="https://www.uc3m.es/Home">University Carlos III of Madrid</a>. More information about this seminar can be found <a href="https://www.uc3m.es/ss/Satellite/Postgrado/es/TextoMixta/1371234989154"> here</a>.
		</p>
		<p>
		<span>&#x25cf;</span> &nbsp; <b>[May 2017]</b> &nbsp; My paper: "Learning and Recognizing Human Action from Skeleton Movement with Deep Residual Neural Networks " has been accepted for oral presentation at the <a href="https://communities.theiet.org/communities/events/item/122/75/15838"> 8th International Conference of Pattern Recognition Systems (ICPRS-17)</a>.		    
		</p>			    
            </td>
          </tr>
        </table>
	      
	      
	      
	<table width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Publications</heading>	    
            </td>
	
	<tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/ICIAR_2019.png'></div>
                <img src='images/ICIAR_2019.png'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }
                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>		    
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.aimiconf.org/iciar19/">
                <papertitle>A Deep Learning Approach for Real-Time 3D Human Action Recognition from Skeletal Data</a>
	      <br> 
	      <font color="red"><strong>Conference Paper</strong></font>    
              <br>
              <strong>Huy Hieu Pham</strong>, 
	      <a href="https://scholar.google.fr/citations?user=yvLitLEAAAAJ&hl=fr">Houssam Salmane</a>,
	      <a href="https://www.researchgate.net/profile/Louahdi_Khoudour">Louahdi Khoudour</a>,
	      <a href="https://www.irit.fr/~Alain.Crouzil/">Alain Crouzil</a>,
	      <a href="https://www.researchgate.net/profile/Pablo_Zegers">Pablo Zegers</a>, and
	      <a href="https://www.uc3m.es/ss/Satellite/UC3MInstitucional/en/FormularioTextoDosColumnas/1371216239282/Very_experienced_fellow">Sergio A Velastin</a>
              <br>
              <em>The 16th International Conference on Image Analysis and Recognition</em>, ICIAR2019, August 27-29, 2019, Waterloo, Canada
              <br>
	      <p></p>
              <p>Building a real-time deep learning-based framework for skeleton-based human action recognition. Application to public transport monitoring.</p>
            </td>
          </tr>
	  </tr>
        </table>  
	      
	<table width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>	  
	<tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/Sensors_2019.png'></div>
                <img src='images/Sensors_2019.png'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }
                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>		    
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.ncbi.nlm.nih.gov/pubmed/31022945">
                <papertitle>Spatio-Temporal Image Representation of 3D Skeletal Movements for View-Invariant Action Recognition with Deep Convolutional Neural Networks</a>
	      <br> 
	      <font color="red"><strong>Journal Paper</strong></font>    
              <br>
              <strong>Huy Hieu Pham</strong>, 
	      <a href="https://scholar.google.fr/citations?user=yvLitLEAAAAJ&hl=fr">Houssam Salmane</a>,
	      <a href="https://www.researchgate.net/profile/Louahdi_Khoudour">Louahdi Khoudour</a>,
	      <a href="https://www.irit.fr/~Alain.Crouzil/">Alain Crouzil</a>,
	      <a href="https://www.researchgate.net/profile/Pablo_Zegers">Pablo Zegers</a>, and
	      <a href="https://www.uc3m.es/ss/Satellite/UC3MInstitucional/en/FormularioTextoDosColumnas/1371216239282/Very_experienced_fellow">Sergio A Velastin</a>
              <br>
              <em>Deep Learning-Based Image Sensors, Intelligent Sensors</em>, Sensors 2019
              <br>
              <a href="https://www.preprints.org/manuscript/201903.0086/v1">Preprint</a> /
	      <a href=" https://doi.org/10.3390/s19081932">DOI</a> /    
              <a href="data/HieuPham_Sensors_2019.bib">BibTeX</a>
	       <p></p>
              <p> A new motion representation called Enhanced-SPMF for human action recognition in videos with deep neural networks.</p>
            </td>
          </tr>
	  </tr>
        </table>  
	 
	<table width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
	<tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/IET_2018.png'></div>
                <img src='images/IET_2018.png'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }
                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>		    
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://digital-library.theiet.org/content/journals/10.1049/iet-cvi.2018.5014">
                <papertitle>Learning to Recognize 3D Human Action from A New Skeleton-based Representation Using Deep Convolutional Neural Networks</papertitle>
              </a>
	      <br> 
	      <font color="red"><strong>Journal Paper</strong></font>    
              <br>
              <strong>Huy Hieu Pham</strong>, 
	      <a href="https://www.researchgate.net/profile/Louahdi_Khoudour">Louahdi Khoudour</a>,
	      <a href="https://www.irit.fr/~Alain.Crouzil/">Alain Crouzil</a>,
	      <a href="https://www.researchgate.net/profile/Pablo_Zegers">Pablo Zegers</a>, and
	      <a href="https://www.uc3m.es/ss/Satellite/UC3MInstitucional/en/FormularioTextoDosColumnas/1371216239282/Very_experienced_fellow">Sergio A Velastin</a>
              <br>
              <em>IET Computer Vision</em>, IET 2018
              <br>
              <a href="https://arxiv.org/pdf/1812.10550.pdf">arXiv</a> /
	      <a href="https://doi.org/10.1049/iet-cvi.2018.5014">DOI</a> /    
              <a href="data/HieuPham_IET_2018.bib">BibTeX</a>
              <p></p>
              <p>Transforming 3D joint coordinates of the human body carried in skeleton sequences into RGB images for human action recognition in videos with deep neural networks.</p>
            </td>
          </tr>
	  </tr>
        </table>  
	  
	<table width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
	<tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/ICIP_2018.png'></div>
                <img src='images/ICIP_2018.png'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }
                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>		    
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/8451404">
                <papertitle>Skeletal Movement to Color Map: A Novel Representation for 3D Action Recognition with Inception Residual Networks</papertitle>
              </a>
	      <br> 
	      <font color="red"><strong>Conference Paper</strong></font>    
              <br>
              <strong>Huy Hieu Pham</strong>, 
	      <a href="https://www.researchgate.net/profile/Louahdi_Khoudour">Louahdi Khoudour</a>,
	      <a href="https://www.irit.fr/~Alain.Crouzil/">Alain Crouzil</a>,
	      <a href="https://www.researchgate.net/profile/Pablo_Zegers">Pablo Zegers</a>, and
	      <a href="https://www.uc3m.es/ss/Satellite/UC3MInstitucional/en/FormularioTextoDosColumnas/1371216239282/Very_experienced_fellow">Sergio A Velastin</a>
              <br>
              <em>The 25th IEEE International Conference on Image Processing</em>, ICIP2018, October 7-10, 2018, Athens, Greece
              <br>
              <a href="https://arxiv.org/pdf/1807.07033.pdf">arXiv</a> /
	      <a href="https://doi.org/10.1109/ICIP.2018.8451404">DOI</a> /    
              <a href="data/HieuPham_ICIP_2018.bib">BibTeX</a>
	      <p></p>
              <p>Proposing a new 3D skeleton-based representation, namely, SPMF (Skeleton Pose-Motion Feature) for video-based human action recogntion with depth sensors.</p>
            </td>
          </tr>
	  </tr>
        </table>  
		  
	<table width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
	<tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/CVIU_figure.png'></div>
                <img src='images/CVIU_figure.png'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }
                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>		    
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.sciencedirect.com/science/article/pii/S1077314218300389">
                <papertitle>Exploiting Deep Residual Networks for Human Action Recognition from Skeletal Data</papertitle>
              </a>
	      <br> 
	      <font color="red"><strong>Journal Paper</strong></font>    
              <br>
              <strong>Huy Hieu Pham</strong>, 
	      <a href="https://www.researchgate.net/profile/Louahdi_Khoudour">Louahdi Khoudour</a>,
	      <a href="https://www.irit.fr/~Alain.Crouzil/">Alain Crouzil</a>,
	      <a href="https://www.researchgate.net/profile/Pablo_Zegers">Pablo Zegers</a>, and
	      <a href="https://www.uc3m.es/ss/Satellite/UC3MInstitucional/en/FormularioTextoDosColumnas/1371216239282/Very_experienced_fellow">Sergio A Velastin</a>
              <br>
              <em>Computer Vision and Image Understanding</em>, CVIU 2018
              <br>
              <a href="https://arxiv.org/pdf/1803.07781.pdf">arXiv</a> /
	      <a href="https://doi.org/10.1016/j.cviu.2018.03.003">DOI</a> /    
	      <a href="https://github.com/huyhieupham/Improved-ResNet-Action-Recognition-Skeletal-Data">Code</a> /
              <a href="data/HieuPham_CVIU_2018.bib">BibTeX</a>
	      <p></p>
              <p>Investigating and applying deep ResNets for human action recognition using skeletal data provided by depth sensors.</p>
            </td>
          </tr>
	  </tr>
        </table>  
		  
	<table width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
	<tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/DefautJENv1.png'></div>
                <img src='images/DefautJENv1.png'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }
                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>		    
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://link.springer.com/article/10.1007/s10921-017-0453-1">
                <papertitle>3D Point Cloud Analysis for Detection and Characterization of Defects on Airplane Exterior Surface</papertitle>
              </a>
	      <br> 
	      <font color="red"><strong>Journal Paper</strong></font>    
              <br>
              Igor Jovančević, <strong>Huy Hieu Pham</strong>, Jean-José Orteu, Rémi Gilblas, Jacques Harvent, Xavier Maurice, and Ludovic Brèthes
              <br>
              <em>Journal of Nondestructive Evaluation</em>, JNE 2017
              <br>
              <a href="https://hal-mines-albi.archives-ouvertes.fr/hal-01622056/document">Full-text PDF</a> /
	      <a href="https://doi.org/10.1007/s10921-017-0453-1">DOI</a> /    
              <a href="data/Jovancevic2017JNE.bib">BibTeX</a>
              <p></p>
              <p>A novel automatic vision-based inspection system that is capable of detecting and characterizing defects on an airplane exterior surface.</p>
            </td>
          </tr>
	  </tr>
        </table>  
		
	<table width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr> 	  
	   <tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/Nuage.png'></div>
                <img src='images/Nuage.png'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }
                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>		    
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://hal.archives-ouvertes.fr/hal-01660998/">
                <papertitle>Détection et Caractérisation de Défauts de Surface par Analyse des Nuages de Points 3D Fournis par un Scanner</papertitle>
              </a>
	      <br> 
	      <font color="red"><strong>Journal Paper</strong></font>    
              <br>
              Igor Jovančević, <strong>Huy Hieu Pham</strong>, Jean-José Orteu, Rémi Gilblas, Jacques Harvent, Xavier Maurice, and Ludovic Brèthes
              <br>
              <em>Journal of Sensors</em>, 2016
              <br>
              <a href="https://hal.archives-ouvertes.fr/hal-01660998/document">Full-text PDF</a> /
              <a href="data/jovancevic:hal2017.bib">BibTeX</a>
              <p></p>
              <p>A system that is able to detect obstacles in indoor environment based on Kinect sensor and 3D-image processing.</p>
            </td>
          </tr>
	  </tr>
        </table>
	  
	<table width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr> 
	<tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/ResNet.png'></div>
                <img src='images/Kinect-Joint.png'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }
                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>		    
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://digital-library.theiet.org/content/conferences/10.1049/cp.2017.0154">
                <papertitle>Learning and Recognizing Human Action from Skeleton Movement with Deep Residual Neural Networks</papertitle>
              </a>
	      <br> 
	      <font color="red"><strong>Conference Paper</strong></font>
              <br>
              <strong>Huy Hieu Pham</strong>, 
	      <a href="https://www.researchgate.net/profile/Louahdi_Khoudour">Louahdi Khoudour</a>,
	      <a href="https://www.irit.fr/~Alain.Crouzil/">Alain Crouzil</a>,
	      <a href="https://www.researchgate.net/profile/Pablo_Zegers">Pablo Zegers</a>, and
	      <a href="https://www.uc3m.es/ss/Satellite/UC3MInstitucional/en/FormularioTextoDosColumnas/1371216239282/Very_experienced_fellow">Sergio A Velastin</a>
              <br>
              <em>The 8th International Conference of Pattern Recognition Systems</em>, ICPRS2017, July 12-13, 2017, Madrid, Spain
              <br>
              <a href="https://arxiv.org/pdf/1803.07780.pdf">arXiv</a> /
	      <a href="https://doi.org/10.1049/cp.2017.0154">DOI</a> /
              <a href="data/HieuPhamCVPRS2017.bib">BibTeX</a>
              <p></p>
              <p>Training Deep Residual Neural Networks to learn and recognize human action from skeleton data provided by Kinect sensor.</p>
            </td>
          </tr>
	   </tr>
        </table>
	  
	<table width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>	
	<tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/obtacle1.png'></div>
                <img src='images/obtacle2.png'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }
                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>		    
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.hindawi.com/journals/js/2016/3754918/abs/">
                <papertitle>Real-time Obstacle Detection System in Indoor Environment for the Visually Impaired Using Microsoft Kinect Sensor</papertitle>
              </a>
	      <br> 
	      <font color="red"><strong>Journal Paper</strong></font>
              <br>
              <strong>Huy Hieu Pham</strong>, 
	      <a href="https://www.mica.edu.vn/perso/Le-Thi-Lan/">Thi Lan Le</a>, and
	      <a href="https://www.omicsonline.org/editor-profile/Nicolas_Vuillerme/">Nicolas Vuillerme</a>
              <br>
              <em>Journal of Sensors</em>, 2016 
		    
              <br>
              <a href="http://downloads.hindawi.com/journals/js/2016/3754918.pdf">Full-text PDF</a> /
              <a href="https://www.hindawi.com/journals/js/2016/3754918/">Full-text HTML </a> /
	      <a href="http://dx.doi.org/10.1155/2016/3754918/>DOI</a> /
              <a href="https://www.youtube.com/watch?v=9IQPptJbO4M">Video</a> /
              <a href="data/HieuPhamSensor2016.bib">BibTeX</a>
              <p></p>
              <p>A system that is able to detect obstacles in indoor environment based on Kinect sensor and 3D-image processing.</p>
            </td>
          </tr>
						   
          </tr>
        </table>
	      
        <table width="100%" align="center" border="0" cellpadding="20"> 
	
            
              <p>
                
                <br>
                <br>
                
                <br>
                <br>
              </p>
            </td>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
              <p align="right">
                <font size="2">
                  Thanks <a href="https://jonbarron.info/">Jon Barron</a> for his awesome open source code.
       
                  </font>
              </p>
            </td>
          </tr>
        </table>	
	      
	
	  
	  
		
        
         <!–– Please delete this script if you use this HTML. ––>
        <script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        <script type="text/javascript">
          try {
            var pageTracker = _gat._getTracker("UA-7580334-1");
            pageTracker._trackPageview();
          } catch (err) {}
        </script>
        </td>
    </tr> 
  </table>
</body>
</html>
							     
							     
